{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers evaluate nltk functools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Vh27chg-f_aH",
        "outputId": "58024163-6f59-4792-9255-24639cd0e11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting evaluate\n",
            "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting functools\n",
            "  Using cached functools-0.5.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Building wheels for collected packages: functools\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for functools (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for functools\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for functools\n",
            "Failed to build functools\n",
            "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (functools)\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keygen -t ed25519 -C \"frank.scagluso@bc.edu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SiTif6tQzOKP",
        "outputId": "eec3c6c9-3110-4e5c-d3ca-c373b3a16812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating public/private ed25519 key pair.\n",
            "Enter file in which to save the key (/root/.ssh/id_ed25519): \n",
            "Created directory '/root/.ssh'.\n",
            "Enter passphrase (empty for no passphrase): \n",
            "Enter same passphrase again: \n",
            "Your identification has been saved in /root/.ssh/id_ed25519\n",
            "Your public key has been saved in /root/.ssh/id_ed25519.pub\n",
            "The key fingerprint is:\n",
            "SHA256:gM1+fYr8U3RlmaRpZ7yccLUu398jdkD5KHc0wIpBA+U frank.scagluso@bc.edu\n",
            "The key's randomart image is:\n",
            "+--[ED25519 256]--+\n",
            "|       .++  . ..+|\n",
            "|     +  ...  o+o=|\n",
            "|    . +  Eo .=oO |\n",
            "|     . . o .ooOoo|\n",
            "|      . S ..oo+=.|\n",
            "|       o . +.+oo.|\n",
            "|        o ..o o..|\n",
            "|         ..  o oo|\n",
            "|          ... o +|\n",
            "+----[SHA256]-----+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat ~/.ssh/id_ed25519.pub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXj6zmo2zRn-",
        "outputId": "b56918be-df22-42a1-de71-5f471eb266b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBwXxuIBPwFkw2yjWl8aJeqr3QY9go8PIUXzqrR5rFls frank.scagluso@bc.edu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh-keyscan github.com >> ~/.ssh/known_hosts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGYGkdRyzUzY",
        "outputId": "a3ad61ed-3103-44a1-8011-43bdd8b5cf48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# github.com:22 SSH-2.0-93587f60\n",
            "# github.com:22 SSH-2.0-93587f60\n",
            "# github.com:22 SSH-2.0-93587f60\n",
            "# github.com:22 SSH-2.0-93587f60\n",
            "# github.com:22 SSH-2.0-93587f60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ssh -T git@github.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWaY7lGCzWry",
        "outputId": "8e198940-adde-4b90-ba8d-34016c178b19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi fscagz! You've successfully authenticated, but GitHub does not provide shell access.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone git@github.com:howellbe02/NLP-Project-Group-3.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijXzcmKozfuG",
        "outputId": "6892a067-e60b-4931-f057-04d723ad7012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NLP-Project-Group-3'...\n",
            "remote: Enumerating objects: 1340, done.\u001b[K\n",
            "remote: Counting objects: 100% (1340/1340), done.\u001b[K\n",
            "remote: Compressing objects: 100% (934/934), done.\u001b[K\n",
            "remote: Total 1340 (delta 647), reused 1083 (delta 401), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (1340/1340), 12.18 MiB | 22.83 MiB/s, done.\n",
            "Resolving deltas: 100% (647/647), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf4Swa2WT823",
        "outputId": "bccaa1af-01c7-4bcd-e8e3-785ace7e88c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Using cached datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhGmlas9UEVq",
        "outputId": "3e247ca3-364e-49fd-e70f-400dcc5716ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.19.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "Installing collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, load_from_disk\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoConfig\n",
        "from functools import partial\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "edkW1A1Qfdnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cihuh80zVxzr",
        "outputId": "d8542c4d-d6f5-4b14-c766-dea229e3de7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wandb\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.19.9\n",
            "    Uninstalling wandb-0.19.9:\n",
            "      Successfully uninstalled wandb-0.19.9\n",
            "Successfully installed wandb-0.19.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu91qw5dy0Al",
        "outputId": "4b0eeb81-4b75-4320-f9b4-9b8adea852c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API Key: 692f8e561cbd80fda1432e206bad1d7c23897a4d"
      ],
      "metadata": {
        "id": "OeznxVq55kvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, load_from_disk\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from functools import partial\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "lFQJXAH3SCxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "UzLWYyyRfMzB",
        "outputId": "650135f8-d3d3-4dd0-e984-0cde279b0c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading processed training dataset from disk...\n",
            "Loading processed test dataset from disk...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='80010' max='80010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80010/80010 2:42:41, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>16002</td>\n",
              "      <td>0.943500</td>\n",
              "      <td>1.323361</td>\n",
              "      <td>0.423077</td>\n",
              "      <td>0.230435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32004</td>\n",
              "      <td>0.765000</td>\n",
              "      <td>1.698274</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.248252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48006</td>\n",
              "      <td>0.627900</td>\n",
              "      <td>2.630059</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.248696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64008</td>\n",
              "      <td>0.396400</td>\n",
              "      <td>3.804136</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.209790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80010</td>\n",
              "      <td>0.281800</td>\n",
              "      <td>4.274959</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>0.208333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1360' max='1360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1360/1360 00:41]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation results: {'eval_loss': 2.630058765411377, 'eval_accuracy': 0.46153846153846156, 'eval_f1': 0.24869565217391304, 'eval_runtime': 41.9119, 'eval_samples_per_second': 129.748, 'eval_steps_per_second': 32.449, 'epoch': 10.0}\n",
            "Training and evaluation completed. Model saved to './best_hierarchical_transformer_model'\n"
          ]
        }
      ],
      "source": [
        "##### THIS FILE NEEDS TO BE IN THE SAME DIRECTORY AS THE DATA DIRECTORY OR UPDATE THE 'BASE_DIR'\n",
        "\n",
        "class HierarchicalTransformer(nn.Module):\n",
        "    def __init__(self, pretrained_model_name, num_classes):\n",
        "        super(HierarchicalTransformer, self).__init__()\n",
        "        self.segment_encoder = AutoModel.from_pretrained(pretrained_model_name) # Segment encoder (pretrained model)\n",
        "        hidden_size = self.segment_encoder.config.hidden_size\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=6,  # As mentioned in the paper\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.document_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3) # Document encoder (transformer layers)\n",
        "\n",
        "        self.num_classes = num_classes # For our purposes this = 4\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        ) # Classification layers\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None, transcript_id=None, **kwargs):\n",
        "        # Process all segments\n",
        "        outputs = self.segment_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token embeddings for each segment\n",
        "        unique_transcripts = list(set(transcript_id))\n",
        "        all_document_embeddings = []\n",
        "        all_labels = []\n",
        "\n",
        "        for trans_id in unique_transcripts:\n",
        "            indices = [i for i, t_id in enumerate(transcript_id) if t_id == trans_id]  # Get indices of segments belonging to this transcript\n",
        "\n",
        "            if not indices:\n",
        "                continue\n",
        "\n",
        "            transcript_embeddings = cls_embeddings[indices]  # Get embeddings, Shape: [num_segments, hidden_size]\n",
        "\n",
        "            document_output = self.document_encoder(transcript_embeddings.unsqueeze(0))  # TransformerEncoder expects [batch_size=1, seq_len=num_segments, hidden_size]\n",
        "\n",
        "            first_token = document_output[0, 0, :]\n",
        "            max_pooled = torch.max(document_output[0], dim=0)[0]  # Max over all segments\n",
        "            doc_representation = torch.cat([first_token, max_pooled], dim=0)\n",
        "            all_document_embeddings.append(doc_representation)\n",
        "\n",
        "            if labels is not None:\n",
        "                all_labels.append(labels[indices[0]])\n",
        "\n",
        "\n",
        "        document_representations = torch.stack(all_document_embeddings)  # Convert lists to tensors\n",
        "        logits = self.classifier(document_representations)  # Classify\n",
        "\n",
        "        # Handle loss calculation (when in training mode)\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            batch_labels = torch.tensor(all_labels, device=logits.device)\n",
        "            loss = loss_fct(logits, batch_labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def load_transcript(file_path, base_path):\n",
        "    with open(f'/content/NLP-Project-Group-3/{file_path}', \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_and_chunk(batch, tokenizer, max_length=512): # Greedy Sentence Chunking like in the paper\n",
        "    output = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"labels\": [],\n",
        "        \"transcript_id\": []\n",
        "    }\n",
        "\n",
        "    for idx in range(len(batch[\"text\"])):\n",
        "        text = batch[\"text\"][idx]\n",
        "        label = batch[\"labels\"][idx]\n",
        "        transcript_id = batch[\"transcript_id\"][idx]\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenizer(sentence, add_special_tokens=False)[\"input_ids\"] # Get token count\n",
        "            sentence_length = len(tokens)\n",
        "\n",
        "            # Check if adding this sentence would exceed the limit\n",
        "            if current_length + sentence_length > max_length - 2:\n",
        "                if current_chunk:  # Process current chunk if not empty\n",
        "                    encoded = tokenizer(\n",
        "                        \" \".join(current_chunk),\n",
        "                        truncation=True,\n",
        "                        max_length=max_length,\n",
        "                        padding=\"max_length\",\n",
        "                        return_tensors=\"pt\"\n",
        "                    ) # Tokenize the whole chunk at once\n",
        "\n",
        "                    output[\"input_ids\"].append(encoded[\"input_ids\"][0])\n",
        "                    output[\"attention_mask\"].append(encoded[\"attention_mask\"][0])\n",
        "                    output[\"labels\"].append(label)\n",
        "                    output[\"transcript_id\"].append(transcript_id)\n",
        "\n",
        "                    current_chunk = []\n",
        "                    current_length = 0\n",
        "\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "        # Handle remaining text\n",
        "        if current_chunk:\n",
        "            encoded = tokenizer(\n",
        "                \" \".join(current_chunk),\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            output[\"input_ids\"].append(encoded[\"input_ids\"][0])\n",
        "            output[\"attention_mask\"].append(encoded[\"attention_mask\"][0])\n",
        "            output[\"labels\"].append(label)\n",
        "            output[\"transcript_id\"].append(transcript_id)\n",
        "\n",
        "    return output\n",
        "\n",
        "def compute_metrics(eval_pred, eval_dataset):\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    logits, labels = eval_pred\n",
        "    transcript_ids = eval_dataset[\"transcript_id\"]\n",
        "\n",
        "    # Get unique transcript IDs that exist in the current evaluation batch\n",
        "    unique_transcripts = {}\n",
        "    for i, t_id in enumerate(transcript_ids):\n",
        "        if t_id not in unique_transcripts and i < len(logits):\n",
        "            unique_transcripts[t_id] = i\n",
        "\n",
        "    # For each unique transcript, take one prediction and one label\n",
        "    doc_predictions = []\n",
        "    doc_labels = []\n",
        "\n",
        "    for t_id, idx in unique_transcripts.items():\n",
        "        # Make sure the index is within bounds\n",
        "        if idx < len(logits):\n",
        "            # Take prediction and label from this segment\n",
        "            pred = np.argmax(logits[idx])\n",
        "            label = labels[idx]\n",
        "\n",
        "            doc_predictions.append(pred)\n",
        "            doc_labels.append(label)\n",
        "\n",
        "    # Calculate metrics on document-level predictions\n",
        "    accuracy = accuracy_metric.compute(predictions=doc_predictions, references=doc_labels)\n",
        "    f1 = f1_metric.compute(predictions=doc_predictions, references=doc_labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "# Custom data collator that ensures proper padding and handles missing keys\n",
        "class CustomDataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features):\n",
        "        # Find the maximum length among all tensors in the batch\n",
        "        max_len = max(feature['input_ids'].shape[0] for feature in features)\n",
        "\n",
        "        # Pad tensors to the maximum length\n",
        "        batch = {}\n",
        "        for key in [\"input_ids\", \"attention_mask\"]:\n",
        "            padded_tensors = [torch.nn.functional.pad(feature[key], (0, max_len - feature[key].shape[0]), value=self.tokenizer.pad_token_id) for feature in features]  # Pad to max_len\n",
        "            batch[key] = torch.stack(padded_tensors)\n",
        "\n",
        "        batch[\"labels\"] = torch.tensor([feature[\"labels\"] for feature in features])\n",
        "        batch[\"transcript_id\"] = [feature[\"transcript_id\"] for feature in features]\n",
        "\n",
        "        return batch\n",
        "\n",
        "def load_data(base_dir):\n",
        "    # CSV FILE PATHS\n",
        "    train_csv_path = \"/content/NLP-Project-Group-3/data/one_hot_targets/train_data.csv\"\n",
        "    test_csv_path = \"/content/NLP-Project-Group-3/data/one_hot_targets/test_data.csv\"\n",
        "\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(train_csv_path)\n",
        "    df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "    # Add transcript ID for identification\n",
        "    df_train[\"transcript_id\"] = df_train[\"file_path\"].apply(lambda x: os.path.basename(x))\n",
        "    df_test[\"transcript_id\"] = df_test[\"file_path\"].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "    # Load transcript content\n",
        "    df_train[\"text\"] = df_train[\"file_path\"].apply(lambda x: load_transcript(x, base_dir))\n",
        "    df_test[\"text\"] = df_test[\"file_path\"].apply(lambda x: load_transcript(x, base_dir))\n",
        "\n",
        "    # Rename label column if it's not already called \"labels\"\n",
        "    df_train = df_train.rename(columns={\"combined_label\": \"labels\"})\n",
        "    df_test = df_test.rename(columns={\"combined_label\": \"labels\"})\n",
        "\n",
        "    # Convert to Hugging Face datasets\n",
        "    dataset_train = Dataset.from_pandas(df_train)\n",
        "    dataset_test = Dataset.from_pandas(df_test)\n",
        "\n",
        "    return dataset_train, dataset_test\n",
        "\n",
        "def process_datasets(dataset_train, dataset_test, tokenizer, base_dir):\n",
        "    columns_to_remove = [\"file_path\", \"text\", \"surprise_pct\", \"volatility_change\", \"report_date\", \"ticker\"]\n",
        "    columns_to_remove = [col for col in columns_to_remove if col in dataset_train.features]\n",
        "\n",
        "    # Processed dataset paths\n",
        "    train_dataset_path = \"/content/gdrive/MyDrive/finbert-nlp/processed_train_dataset\"\n",
        "    test_dataset_path = \"/content/gdrive/MyDrive/finbert-nlp/processed_test_dataset\"\n",
        "\n",
        "    # Define tokenizer function\n",
        "    max_seq_length = 512\n",
        "    tokenize_func = partial(tokenize_and_chunk, tokenizer=tokenizer, max_length=max_seq_length)\n",
        "\n",
        "    # Process and save training dataset\n",
        "    if not os.path.exists(train_dataset_path):\n",
        "        print(\"Processing training dataset...\")\n",
        "        dataset_train_chunked = dataset_train.map(\n",
        "            tokenize_func,\n",
        "            batched=True,\n",
        "            batch_size=8,\n",
        "            remove_columns=columns_to_remove,\n",
        "            num_proc=8,\n",
        "            load_from_cache_file=True\n",
        "        )\n",
        "        dataset_train_chunked.save_to_disk(train_dataset_path)\n",
        "    else:\n",
        "        print(\"Loading processed training dataset from disk...\")\n",
        "        dataset_train_chunked = load_from_disk(train_dataset_path)\n",
        "\n",
        "    # Process and save test dataset\n",
        "    if not os.path.exists(test_dataset_path):\n",
        "        print(\"Processing test dataset...\")\n",
        "        dataset_test_chunked = dataset_test.map(\n",
        "            tokenize_func,\n",
        "            batched=True,\n",
        "            batch_size=8,\n",
        "            remove_columns=columns_to_remove,\n",
        "            num_proc=8,\n",
        "            load_from_cache_file=True\n",
        "        )\n",
        "        dataset_test_chunked.save_to_disk(test_dataset_path)\n",
        "    else:\n",
        "        print(\"Loading processed test dataset from disk...\")\n",
        "        dataset_test_chunked = load_from_disk(test_dataset_path)\n",
        "\n",
        "    # Set dataset format to PyTorch tensors\n",
        "    dataset_train_chunked.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\", \"transcript_id\"]\n",
        "    )\n",
        "    dataset_test_chunked.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\", \"transcript_id\"]\n",
        "    )\n",
        "\n",
        "    return dataset_train_chunked, dataset_test_chunked\n",
        "\n",
        "def setup_model_and_trainer(model_name, dataset_train_chunked, dataset_test_chunked, tokenizer, device):\n",
        "    # Create HierarchicalTransformer model\n",
        "    model = HierarchicalTransformer(\n",
        "        pretrained_model_name=model_name,\n",
        "        num_classes=4  # Make sure this matches your actual number of classes\n",
        "    )\n",
        "\n",
        "    # Add config attribute needed by Trainer\n",
        "    model.config = type('obj', (object,), {\n",
        "        'num_labels': model.num_classes\n",
        "    })\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./hierarchical_transformer_output\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "        learning_rate=1e-5,\n",
        "        weight_decay=0.01,\n",
        "        # Replace 'evaluation_strategy' with 'eval_strategy'\n",
        "        eval_strategy=\"steps\",            # Evaluate during training\n",
        "        eval_steps=0.2,                         # Evaluate every 20% of training\n",
        "        # Replace 'save_strategy' with 'save_strategy'\n",
        "        save_strategy=\"steps\",                  # Save at the same intervals\n",
        "        save_steps=0.2,                         # Save every 20% of training\n",
        "        load_best_model_at_end=True,            # Load best model at end\n",
        "        metric_for_best_model=\"f1\",              # Use F1 score for best model\n",
        "        report_to=\"none\",  # Disable wandb reporting by default\n",
        "        resume_from_checkpoint=\"/content/best_hierarchical_transformer_model/model.safetensors\"\n",
        "    )\n",
        "\n",
        "    # Use our improved data collator to ensure proper padding and handle missing keys\n",
        "    data_collator = CustomDataCollator(tokenizer)\n",
        "\n",
        "    def compute_metrics_wrapper(eval_pred): # Some BS to avoid nonsense with how this trainer works, there's always something\n",
        "        return compute_metrics(eval_pred, dataset_test_chunked)\n",
        "\n",
        "    # Initialize standard Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_train_chunked,\n",
        "        eval_dataset=dataset_test_chunked,\n",
        "        compute_metrics=compute_metrics_wrapper,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    return trainer\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model_name = \"yiyanghkust/finbert-tone\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load and process data\n",
        "    BASE_DIR = os.getcwd()\n",
        "    dataset_train, dataset_test = load_data(BASE_DIR)\n",
        "    dataset_train_chunked, dataset_test_chunked = process_datasets(\n",
        "        dataset_train, dataset_test, tokenizer, BASE_DIR\n",
        "    )\n",
        "\n",
        "    trainer = setup_model_and_trainer(\n",
        "        model_name, dataset_train_chunked, dataset_test_chunked, tokenizer, device\n",
        "    )\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation results:\", eval_results)\n",
        "\n",
        "    trainer.save_model(\"./best_hierarchical_transformer_model\")\n",
        "    print(\"Training and evaluation completed. Model saved to './best_hierarchical_transformer_model'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RIkzgUEQVcqs",
        "outputId": "6d082560-686a-4f3d-e782-db5af43708fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/16002_output.zip /content/hierarchical_transformer_output/checkpoint-16002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1pyfnG_XseB",
        "outputId": "9d1311fd-d772-4f8b-ae1c-337c4df98f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/ (stored 0%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/tokenizer_config.json (deflated 74%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/tokenizer.json (deflated 70%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/rng_state.pth (deflated 25%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/training_args.bin (deflated 51%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/vocab.txt (deflated 50%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/optimizer.pt (deflated 18%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/model.safetensors (deflated 7%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/trainer_state.json (deflated 73%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-16002/scheduler.pt (deflated 55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/32004_output.zip /content/hierarchical_transformer_output/checkpoint-32004\n",
        "!zip -r /content/48006_output.zip /content/hierarchical_transformer_output/checkpoint-48006\n",
        "!zip -r /content/64008_output.zip /content/hierarchical_transformer_output/checkpoint-64008\n",
        "!zip -r /content/80010_output.zip /content/hierarchical_transformer_output/checkpoint-80010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCy2W35uX2PN",
        "outputId": "a43bdd96-5cb0-42c7-dbe1-029f89d31787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/ (stored 0%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/tokenizer_config.json (deflated 74%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/tokenizer.json (deflated 70%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/rng_state.pth (deflated 25%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/training_args.bin (deflated 51%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/vocab.txt (deflated 50%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/optimizer.pt (deflated 18%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/model.safetensors (deflated 7%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/trainer_state.json (deflated 75%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-32004/scheduler.pt (deflated 56%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/ (stored 0%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/tokenizer_config.json (deflated 74%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/tokenizer.json (deflated 70%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/rng_state.pth (deflated 25%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/training_args.bin (deflated 51%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/vocab.txt (deflated 50%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/optimizer.pt (deflated 18%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/model.safetensors (deflated 7%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/trainer_state.json (deflated 76%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-48006/scheduler.pt (deflated 55%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/64008_output.zip /content/hierarchical_transformer_output/checkpoint-64008\n",
        "!zip -r /content/80010_output.zip /content/hierarchical_transformer_output/checkpoint-80010"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuBD2W8kZKss",
        "outputId": "aa198b28-7afb-4f9d-e081-b55b23f7a3ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/ (stored 0%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/tokenizer_config.json (deflated 74%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/tokenizer.json (deflated 70%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/rng_state.pth (deflated 25%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/training_args.bin (deflated 51%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/vocab.txt (deflated 50%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/optimizer.pt (deflated 18%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/model.safetensors (deflated 7%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/trainer_state.json (deflated 76%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-64008/scheduler.pt (deflated 55%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/ (stored 0%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/tokenizer_config.json (deflated 74%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/tokenizer.json (deflated 70%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/rng_state.pth (deflated 25%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/training_args.bin (deflated 51%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/vocab.txt (deflated 50%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/optimizer.pt (deflated 18%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/model.safetensors (deflated 7%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/trainer_state.json (deflated 76%)\n",
            "  adding: content/hierarchical_transformer_output/checkpoint-80010/scheduler.pt (deflated 56%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp /content/16002_output.zip ./gdrive/MyDrive/newou\n",
        "#!cp /content/32004_output.zip ./gdrive/MyDrive/newou\n",
        "#!cp /content/48006_output.zip ./gdrive/MyDrive/newou\n",
        "!cp /content/64008_output.zip ./gdrive/MyDrive/newou\n",
        "!cp /content/80010_output.zip ./gdrive/MyDrive/newou"
      ],
      "metadata": {
        "id": "lGM3F3UgYLFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/best-hierarchical_transformer_model.zip ./gdrive/MyDrive/newou"
      ],
      "metadata": {
        "id": "fmxR5oEfao5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall wandb -y\n",
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O9S8dEBmV9Nw",
        "outputId": "6fa85763-d1d6-4abc-f1a4-d9ba99ca0d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: wandb 0.19.10\n",
            "Uninstalling wandb-0.19.10:\n",
            "  Successfully uninstalled wandb-0.19.10\n",
            "Collecting wandb\n",
            "  Using cached wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Using cached wandb-0.19.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "Installing collected packages: wandb\n",
            "Successfully installed wandb-0.19.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/best-hierarchical_transformer_model.zip /content/best_hierarchical_transformer_model\n",
        "!zip -r /content/hierarchical_transformer_output.zip /content/hierarchical_transformer_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gPYqXXxNezg8",
        "outputId": "2394321c-b186-47d6-a805-52b7d727a44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/best_hierarchical_transformer_model/ (stored 0%)\n",
            "updating: content/best_hierarchical_transformer_model/tokenizer_config.json (deflated 74%)\n",
            "updating: content/best_hierarchical_transformer_model/tokenizer.json (deflated 70%)\n",
            "updating: content/best_hierarchical_transformer_model/training_args.bin (deflated 51%)\n",
            "updating: content/best_hierarchical_transformer_model/vocab.txt (deflated 50%)\n",
            "updating: content/best_hierarchical_transformer_model/special_tokens_map.json (deflated 42%)\n",
            "updating: content/best_hierarchical_transformer_model/model.safetensors (deflated 7%)\n",
            "updating: content/hierarchical_transformer_output/ (stored 0%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/ (stored 0%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/tokenizer_config.json (deflated 74%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/tokenizer.json (deflated 70%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/rng_state.pth (deflated 25%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/training_args.bin (deflated 51%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/vocab.txt (deflated 50%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/special_tokens_map.json (deflated 42%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/optimizer.pt (deflated 17%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/model.safetensors (deflated 7%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/trainer_state.json (deflated 76%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-51208/scheduler.pt (deflated 56%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/ (stored 0%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/tokenizer_config.json (deflated 74%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/tokenizer.json (deflated 70%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/rng_state.pth (deflated 25%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/training_args.bin (deflated 51%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/vocab.txt (deflated 50%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/special_tokens_map.json (deflated 42%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/optimizer.pt (deflated 17%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/model.safetensors (deflated 7%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/trainer_state.json (deflated 75%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-25604/scheduler.pt (deflated 56%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/ (stored 0%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/tokenizer_config.json (deflated 74%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/tokenizer.json (deflated 70%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/rng_state.pth (deflated 25%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/training_args.bin (deflated 51%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/vocab.txt (deflated 50%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/special_tokens_map.json (deflated 42%)\n",
            "updating: content/hierarchical_transformer_output/checkpoint-38406/optimizer.pt\n",
            "\n",
            "\n",
            "zip error: Interrupted (aborting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"/content/best-hierarchical_transformer_model.zip\")\n",
        "files.download(\"/content/hierarchical_transformer_output.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "58KJLqNOg3ea",
        "outputId": "0dce90f4-4938-4cee-9030-37bdcd0743b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_509ec01e-9b45-4b58-925a-8c2de1fbc504\", \"best-hierarchical_transformer_model.zip\", 470263119)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f62beea3-b97e-46bf-84f9-d1a9ed16ee04\", \"hierarchical_transformer_output.zip\", 6526664050)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.download(\"/content/hierarchical_transformer_output.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "839a941d-45f3-466a-8621-4ae2bb871a85",
        "id": "Og_U8cXUgXLt"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6d06fc9d-1324-409b-aa37-a900bd8a601d\", \"hierarchical_transformer_output.zip\", 6526664050)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/hierarchical_transformer_output/checkpoint-38406/model.safetensors ./gdrive/MyDrive/newou"
      ],
      "metadata": {
        "id": "vhNhgP8wjGsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/hierarchical_transformer_output/checkpoint-12802 ./gdrive/MyDrive/newou"
      ],
      "metadata": {
        "id": "CcjXZvHHjiJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -R /content/best_hierarchical_transformer_model ./gdrive/MyDrive/newou"
      ],
      "metadata": {
        "id": "_NgvfoXUkFd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "TD0ursDfhSXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_from_disk\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from safetensors.torch import load_file\n",
        "import pandas as pd\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Model and data loading\n",
        "model_name = \"yiyanghkust/finbert-tone\"\n",
        "model_path = \"/content/NLP-Project-Group-3/best_hierarchical_transformer_model/model.safetensors\"\n",
        "test_dataset_path = \"/content/gdrive/MyDrive/finbert-nlp/processed_test_dataset\"\n",
        "test_csv_path = \"/content/NLP-Project-Group-3/data/one_hot_targets/test_data.csv\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4, ignore_mismatched_sizes=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "state_dict = load_file(model_path)\n",
        "model.classifier = torch.nn.Linear(model.classifier.in_features, 4)  # Update classifier\n",
        "for key in state_dict:\n",
        "    state_dict[key] = state_dict[key].to(device)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model.to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load test dataset\n",
        "dataset_test_chunked = load_from_disk(test_dataset_path)\n",
        "df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Function to load the transcript text\n",
        "def load_transcript(file_path):\n",
        "    with open(f'/content/NLP-Project-Group-3/{file_path}', \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# Create a dictionary mapping transcript_id to the full transcript text\n",
        "transcript_texts = {os.path.basename(file_path): load_transcript(file_path) for file_path in df_test[\"file_path\"]}\n",
        "\n",
        "# Add the 'text' column back to the dataset\n",
        "def add_text_column(example):\n",
        "    example[\"text\"] = transcript_texts[example[\"transcript_id\"]]\n",
        "    return example\n",
        "\n",
        "dataset_test_chunked = dataset_test_chunked.map(add_text_column)\n",
        "\n",
        "# Prediction function\n",
        "def predict(text):\n",
        "    encoding = tokenizer(text, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "    return predictions[0]  # Return single prediction value\n",
        "\n",
        "# Test functions\n",
        "def test_first_512(example):\n",
        "    return predict(example[\"text\"][:512])\n",
        "\n",
        "def test_last_512(example):\n",
        "    return predict(example[\"text\"][-512:])\n",
        "\n",
        "def test_random_512(example):\n",
        "    text = example[\"text\"]\n",
        "    start_idx = np.random.randint(0, len(text) - 512) if len(text) > 512 else 0\n",
        "    return predict(text[start_idx:start_idx + 512])\n",
        "\n",
        "def test_mean_pooling(example):\n",
        "    chunks = [example[\"text\"][i:i + 512] for i in range(0, len(example[\"text\"]), 512)]\n",
        "\n",
        "    # Batch the chunks (e.g., in batches of 32)\n",
        "    batch_size = 32\n",
        "    all_predictions = []  # Store predictions for all chunks\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i : i + batch_size]\n",
        "\n",
        "        # Tokenize and get predictions for the batch\n",
        "        encoding = tokenizer(batch, truncation=True, max_length=512, padding=True, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoding)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_predictions.extend(predictions)  # Add batch predictions to the list\n",
        "\n",
        "    # Calculate mean after processing all chunks\n",
        "    return int(round(np.mean(all_predictions)))\n",
        "\n",
        "\n",
        "def test_max_pooling(example):\n",
        "    chunks = [example[\"text\"][i:i + 512] for i in range(0, len(example[\"text\"]), 512)]\n",
        "\n",
        "    # Batch the chunks (e.g., in batches of 32)\n",
        "    batch_size = 32\n",
        "    all_predictions = []  # Store predictions for all chunks\n",
        "\n",
        "    for i in range(0, len(chunks), batch_size):\n",
        "        batch = chunks[i : i + batch_size]\n",
        "\n",
        "        # Tokenize and get predictions for the batch\n",
        "        encoding = tokenizer(batch, truncation=True, max_length=512, padding=True, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoding)\n",
        "        logits = outputs.logits\n",
        "        predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
        "        all_predictions.extend(predictions)  # Add batch predictions to the list\n",
        "\n",
        "    # Calculate max after processing all chunks\n",
        "    return int(np.max(all_predictions))\n",
        "\n",
        "# Run tests and store results\n",
        "results = defaultdict(list)\n",
        "for example in dataset_test_chunked:\n",
        "    transcript_id = example[\"transcript_id\"]\n",
        "    results[transcript_id].append({\n",
        "        \"first_512\": test_first_512(example),\n",
        "        \"last_512\": test_last_512(example),\n",
        "        \"random_512\": test_random_512(example),\n",
        "        #\"mean_pooling\": test_mean_pooling(example),\n",
        "        #\"max_pooling\": test_max_pooling(example),\n",
        "    })\n",
        "\n",
        "# Run tests and store results\n",
        "all_predictions = defaultdict(list)\n",
        "ground_truth_labels = []\n",
        "for example in dataset_test_chunked:\n",
        "    all_predictions[\"first_512\"].append(test_first_512(example))\n",
        "    all_predictions[\"last_512\"].append(test_last_512(example))\n",
        "    all_predictions[\"random_512\"].append(test_random_512(example))\n",
        "    all_predictions[\"mean_pooling\"].append(test_mean_pooling(example))\n",
        "    all_predictions[\"max_pooling\"].append(test_max_pooling(example))\n",
        "    ground_truth_labels.append(example[\"labels\"])\n",
        "\n",
        "# Calculate and print overall metrics\n",
        "print(\"Overall Metrics for Test Dataset:\")\n",
        "for method in [\"first_512\", \"last_512\", \"random_512\", \"mean_pooling\", \"max_pooling\"]:\n",
        "    predictions = all_predictions[method]\n",
        "    accuracy = np.mean(np.array(predictions) == np.array(ground_truth_labels))\n",
        "    precision = precision_score(ground_truth_labels, predictions, average='macro')\n",
        "    recall = recall_score(ground_truth_labels, predictions, average='macro')\n",
        "    f1 = f1_score(ground_truth_labels, predictions, average='macro')\n",
        "\n",
        "    print(f\"  {method}:\")\n",
        "    print(f\"    Accuracy: {accuracy}\")\n",
        "    print(f\"    Precision (macro): {precision}\")\n",
        "    print(f\"    Recall (macro): {recall}\")\n",
        "    print(f\"    F1-score (macro): {f1}\")\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waUllNfThUQ5",
        "outputId": "099bc26d-6cd7-4eee-8e21-1e7582b54603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at yiyanghkust/finbert-tone and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Metrics for Test Dataset:\n",
            "  first_512:\n",
            "    Accuracy: 0.03972048547260022\n",
            "    Precision (macro): 0.01037463976945245\n",
            "    Recall (macro): 0.25\n",
            "    F1-score (macro): 0.01992252351964582\n",
            "--------------------\n",
            "  last_512:\n",
            "    Accuracy: 0.06969474071349761\n",
            "    Precision (macro): 0.26023696682464453\n",
            "    Recall (macro): 0.2670146137787056\n",
            "    F1-score (macro): 0.051529377306443164\n",
            "--------------------\n",
            "  random_512:\n",
            "    Accuracy: 0.2898124310408238\n",
            "    Precision (macro): 0.24934930013773055\n",
            "    Recall (macro): 0.2591120496394599\n",
            "    F1-score (macro): 0.20529782093542384\n",
            "--------------------\n",
            "  mean_pooling:\n",
            "    Accuracy: 0.3653916881206326\n",
            "    Precision (macro): 0.1043811725152343\n",
            "    Recall (macro): 0.20741127348643007\n",
            "    F1-score (macro): 0.13887335756220295\n",
            "--------------------\n",
            "  max_pooling:\n",
            "    Accuracy: 0.42386906951084957\n",
            "    Precision (macro): 0.10596726737771239\n",
            "    Recall (macro): 0.25\n",
            "    F1-score (macro): 0.1488441172672091\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#STOP"
      ],
      "metadata": {
        "id": "oT7IgmfkVdNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HierarchicalTransformer(nn.Module):\n",
        "    def __init__(self, pretrained_model_name, num_classes):\n",
        "        super(HierarchicalTransformer, self).__init__()\n",
        "        self.segment_encoder = AutoModel.from_pretrained(pretrained_model_name) # Segment encoder (pretrained model)\n",
        "        hidden_size = self.segment_encoder.config.hidden_size\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=6,  # As mentioned in the paper\n",
        "            dim_feedforward=2048,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.document_encoder = nn.TransformerEncoder(encoder_layer, num_layers=3) # Document encoder (transformer layers)\n",
        "\n",
        "        self.num_classes = num_classes # For our purposes this = 4\n",
        "\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, num_classes)\n",
        "        ) # Classification layers\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None, transcript_id=None, **kwargs):\n",
        "        # Process all segments\n",
        "        outputs = self.segment_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :]  # Extract [CLS] token embeddings for each segment\n",
        "        unique_transcripts = list(set(transcript_id))\n",
        "        all_document_embeddings = []\n",
        "        all_labels = []\n",
        "\n",
        "        for trans_id in unique_transcripts:\n",
        "            indices = [i for i, t_id in enumerate(transcript_id) if t_id == trans_id]  # Get indices of segments belonging to this transcript\n",
        "\n",
        "            if not indices:\n",
        "                continue\n",
        "\n",
        "            transcript_embeddings = cls_embeddings[indices]  # Get embeddings, Shape: [num_segments, hidden_size]\n",
        "\n",
        "            document_output = self.document_encoder(transcript_embeddings.unsqueeze(0))  # TransformerEncoder expects [batch_size=1, seq_len=num_segments, hidden_size]\n",
        "\n",
        "            first_token = document_output[0, 0, :]\n",
        "            max_pooled = torch.max(document_output[0], dim=0)[0]  # Max over all segments\n",
        "            doc_representation = torch.cat([first_token, max_pooled], dim=0)\n",
        "            all_document_embeddings.append(doc_representation)\n",
        "\n",
        "            if labels is not None:\n",
        "                all_labels.append(labels[indices[0]])\n",
        "\n",
        "        if not all_document_embeddings:\n",
        "            raise ValueError(\"No valid document embeddings were created\")\n",
        "\n",
        "        document_representations = torch.stack(all_document_embeddings)  # Convert lists to tensors\n",
        "        logits = self.classifier(document_representations)  # Classify\n",
        "\n",
        "        # Handle loss calculation (when in training mode)\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            batch_labels = torch.tensor(all_labels, device=logits.device)\n",
        "            loss = loss_fct(logits, batch_labels)\n",
        "            return {\"loss\": loss, \"logits\": logits}\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "def load_transcript(file_path, base_path):\n",
        "    try:\n",
        "        with open(f'/content/NLP-Project-Group-3/{file_path}', \"r\", encoding=\"utf-8\") as f:\n",
        "          return f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading transcript {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_and_chunk(batch, tokenizer, max_length=512): # Greedy Sentence Chunking like in the paper\n",
        "    output = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"labels\": [],\n",
        "        \"transcript_id\": []\n",
        "    }\n",
        "\n",
        "    for idx in range(len(batch[\"text\"])):\n",
        "        text = batch[\"text\"][idx]\n",
        "        label = batch[\"labels\"][idx]\n",
        "        transcript_id = batch[\"transcript_id\"][idx]\n",
        "\n",
        "        sentences = sent_tokenize(text)\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            tokens = tokenizer(sentence, add_special_tokens=False)[\"input_ids\"] # Get token count\n",
        "            sentence_length = len(tokens)\n",
        "\n",
        "            # Check if adding this sentence would exceed the limit\n",
        "            if current_length + sentence_length > max_length - 2:\n",
        "                if current_chunk:  # Process current chunk if not empty\n",
        "                    encoded = tokenizer(\n",
        "                        \" \".join(current_chunk),\n",
        "                        truncation=True,\n",
        "                        max_length=max_length,\n",
        "                        padding=\"max_length\",\n",
        "                        return_tensors=\"pt\"\n",
        "                    ) # Tokenize the whole chunk at once\n",
        "\n",
        "                    output[\"input_ids\"].append(encoded[\"input_ids\"][0])\n",
        "                    output[\"attention_mask\"].append(encoded[\"attention_mask\"][0])\n",
        "                    output[\"labels\"].append(label)\n",
        "                    output[\"transcript_id\"].append(transcript_id)\n",
        "\n",
        "                    current_chunk = []\n",
        "                    current_length = 0\n",
        "\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "\n",
        "        # Handle remaining text\n",
        "        if current_chunk:\n",
        "            encoded = tokenizer(\n",
        "                \" \".join(current_chunk),\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                padding=\"max_length\",\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            output[\"input_ids\"].append(encoded[\"input_ids\"][0])\n",
        "            output[\"attention_mask\"].append(encoded[\"attention_mask\"][0])\n",
        "            output[\"labels\"].append(label)\n",
        "            output[\"transcript_id\"].append(transcript_id)\n",
        "\n",
        "    return output\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    accuracy_metric = evaluate.load(\"accuracy\")\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy[\"accuracy\"],\n",
        "        \"f1\": f1[\"f1\"],\n",
        "    }\n",
        "\n",
        "# Custom data collator that ensures proper padding and handles missing keys\n",
        "class CustomDataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = {}\n",
        "        for key in [\"input_ids\", \"attention_mask\"]:\n",
        "            batch[key] = torch.stack([feature[key] for feature in features]) # Stack tensors for input_ids and attention_mask\n",
        "\n",
        "        batch[\"labels\"] = torch.tensor([feature[\"labels\"] for feature in features]) # Convert labels to tensor\n",
        "        batch[\"transcript_id\"] = [feature[\"transcript_id\"] for feature in features] # Pass through transcript_id as list\n",
        "\n",
        "        return batch\n",
        "\n",
        "def load_data(base_dir):\n",
        "    # CSV FILE PATHS\n",
        "\n",
        "    train_csv_path = \"/content/NLP-Project-Group-3/data/one_hot_targets/train_data.csv\"\n",
        "    test_csv_path  = \"/content/NLP-Project-Group-3/data/one_hot_targets/test_data.csv\"\n",
        "    # Load data\n",
        "    df_train = pd.read_csv(train_csv_path)\n",
        "    df_test = pd.read_csv(test_csv_path)\n",
        "\n",
        "    # Add transcript ID for identification\n",
        "    df_train[\"transcript_id\"] = df_train[\"file_path\"].apply(lambda x: os.path.basename(x))\n",
        "    df_test[\"transcript_id\"] = df_test[\"file_path\"].apply(lambda x: os.path.basename(x))\n",
        "\n",
        "    # Load transcript content\n",
        "    df_train[\"text\"] = df_train[\"file_path\"].apply(lambda x: load_transcript(x, base_dir))\n",
        "    df_test[\"text\"] = df_test[\"file_path\"].apply(lambda x: load_transcript(x, base_dir))\n",
        "\n",
        "    # Rename label column if it's not already called \"labels\"\n",
        "    df_train = df_train.rename(columns={\"combined_label\": \"labels\"})\n",
        "    df_test = df_test.rename(columns={\"combined_label\": \"labels\"})\n",
        "\n",
        "    # Convert to Hugging Face datasets\n",
        "    dataset_train = Dataset.from_pandas(df_train)\n",
        "    dataset_test = Dataset.from_pandas(df_test)\n",
        "\n",
        "    return dataset_train, dataset_test\n",
        "\n",
        "def process_datasets(dataset_train, dataset_test, tokenizer, base_dir):\n",
        "    columns_to_remove = [\"file_path\", \"text\", \"surprise_pct\", \"volatility_change\", \"report_date\", \"ticker\"]\n",
        "    columns_to_remove = [col for col in columns_to_remove if col in dataset_train.features]\n",
        "\n",
        "    # Processed dataset paths\n",
        "    train_dataset_path = \"/content/gdrive/MyDrive/finbert-nlp/processed_train_dataset\"\n",
        "    test_dataset_path  = \"/content/gdrive/MyDrive/finbert-nlp/processed_test_dataset\"\n",
        "\n",
        "    # Define tokenizer function\n",
        "    max_seq_length = 512\n",
        "    tokenize_func = partial(tokenize_and_chunk, tokenizer=tokenizer, max_length=max_seq_length)\n",
        "\n",
        "    # Process and save training dataset\n",
        "    if not os.path.exists(train_dataset_path):\n",
        "        print(\"Processing training dataset...\")\n",
        "        dataset_train_chunked = dataset_train.map(\n",
        "            tokenize_func,\n",
        "            batched=True,\n",
        "            batch_size=8,\n",
        "            remove_columns=columns_to_remove,\n",
        "            num_proc=8,\n",
        "            load_from_cache_file=True\n",
        "        )\n",
        "        dataset_train_chunked.save_to_disk(train_dataset_path)\n",
        "    else:\n",
        "        print(\"Loading processed training dataset from disk...\")\n",
        "        dataset_train_chunked = load_from_disk(train_dataset_path)\n",
        "\n",
        "    # Process and save test dataset\n",
        "    if not os.path.exists(test_dataset_path):\n",
        "        print(\"Processing test dataset...\")\n",
        "        dataset_test_chunked = dataset_test.map(\n",
        "            tokenize_func,\n",
        "            batched=True,\n",
        "            batch_size=8,\n",
        "            remove_columns=columns_to_remove,\n",
        "            num_proc=8,\n",
        "            load_from_cache_file=True\n",
        "        )\n",
        "        dataset_test_chunked.save_to_disk(test_dataset_path)\n",
        "    else:\n",
        "        print(\"Loading processed test dataset from disk...\")\n",
        "        dataset_test_chunked = load_from_disk(test_dataset_path)\n",
        "\n",
        "    # Set dataset format to PyTorch tensors\n",
        "    dataset_train_chunked.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\", \"transcript_id\"]\n",
        "    )\n",
        "    dataset_test_chunked.set_format(\n",
        "        type=\"torch\",\n",
        "        columns=[\"input_ids\", \"attention_mask\", \"labels\", \"transcript_id\"]\n",
        "    )\n",
        "\n",
        "    return dataset_train_chunked, dataset_test_chunked\n",
        "\n",
        "def setup_model_and_trainer(model_name, dataset_train_chunked, dataset_test_chunked, tokenizer, device):\n",
        "    # Create HierarchicalTransformer model\n",
        "    model = HierarchicalTransformer(\n",
        "        pretrained_model_name=model_name,\n",
        "        num_classes=4  # Make sure this matches your actual number of classes\n",
        "    )\n",
        "\n",
        "    # Get the configuration from the segment encoder (AutoModel)\n",
        "    config = model.segment_encoder.config\n",
        "\n",
        "    # Update the num_labels in the configuration\n",
        "    config.num_labels = model.num_classes\n",
        "\n",
        "    # Assign the updated configuration back to the model\n",
        "    model.config = config\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./hierarchical_transformer_output\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_steps=50,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\"\n",
        "    )\n",
        "\n",
        "    # Use our improved data collator to ensure proper padding and handle missing keys\n",
        "    data_collator = CustomDataCollator(tokenizer)\n",
        "\n",
        "    # Initialize standard Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_train_chunked,\n",
        "        eval_dataset=dataset_test_chunked,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=data_collator\n",
        "    )\n",
        "\n",
        "    return trainer\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model_name = \"yiyanghkust/finbert-tone\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Load and process data\n",
        "    os.chdir('/content/NLP-Project-Group-3')\n",
        "    BASE_DIR = os.getcwd()\n",
        "    dataset_train, dataset_test = load_data(BASE_DIR)\n",
        "    dataset_train_chunked, dataset_test_chunked = process_datasets(\n",
        "        dataset_train, dataset_test, tokenizer, BASE_DIR\n",
        "    )\n",
        "\n",
        "    trainer = setup_model_and_trainer(\n",
        "        model_name, dataset_train_chunked, dataset_test_chunked, tokenizer, device\n",
        "    )\n",
        "    trainer.train()\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Evaluation results:\", eval_results)\n",
        "\n",
        "    trainer.save_model(\"./best_hierarchical_transformer_model\")\n",
        "    print(\"Training and evaluation completed. Model saved to './best_hierarchical_transformer_model'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "omPcC6kx4AU0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "f7bba4b9-31d8-4f77-9359-42978e931249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading processed training dataset from disk...\n",
            "Loading processed test dataset from disk...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3532007943a3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-3532007943a3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    315\u001b[0m     )\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m     trainer = setup_model_and_trainer(\n\u001b[0m\u001b[1;32m    318\u001b[0m         \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_chunked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test_chunked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-18-3532007943a3>\u001b[0m in \u001b[0;36msetup_model_and_trainer\u001b[0;34m(model_name, dataset_train_chunked, dataset_test_chunked, tokenizer, device)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msetup_model_and_trainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_train_chunked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_test_chunked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Create HierarchicalTransformer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     model = HierarchicalTransformer(\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m  \u001b[0;31m# Make sure this matches your actual number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-3532007943a3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pretrained_model_name, num_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHierarchicalTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Segment encoder (pretrained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         trust_remote_code = resolve_trust_remote_code(\n\u001b[1;32m    550\u001b[0m             \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_local_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_remote_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         mapping_keys = [\n\u001b[0m\u001b[1;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         mapping_keys = [\n\u001b[0;32m--> 788\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_attr_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\".{module_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transformers.models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattribute_from_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;31m# Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/timm_wrapper/configuration_timm_wrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_timm_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageNetInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfer_imagenet_subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mset_exportable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mset_exportable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m from .models import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcreate_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlist_models\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlist_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbyoanet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbyobnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcait\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/models/beit.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIMAGENET_DEFAULT_MEAN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMAGENET_DEFAULT_STD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatchEmbed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSwiGLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLayerNorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fused_attn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresample_patch_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample_abs_pos_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_rel_pos_bias_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresolve_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolve_model_data_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableImageDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAugMixDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_info\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomDatasetInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/readers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreader_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimg_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/readers/reader_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreader_image_folder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReaderImageFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreader_image_in_tar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReaderImageInTar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/data/readers/reader_image_folder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtimm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnatural_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_map\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_class_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodel_ema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelEma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelEmaV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelEmaV3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mupdate_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_outdir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/timm/utils/summary.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Configure the logger as early as possible for consistent behavior.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwb_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_wb_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0m_wb_logging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_wandb_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_alerts\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlertLevel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_init\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_attach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_login\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mwandb_require\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_login\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryDisabled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtelemetry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeprecated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/backend/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface_queue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterfaceQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouter_queue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessageQueueRouter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_internal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings_static\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSettingsStatic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmailbox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMailbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface_queue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterfaceQueue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_util\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/handler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhandler_util\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_metadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_watcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msettings_static\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSettingsStatic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_monitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSystemMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/tb_watcher.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minternal_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/run.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_globals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_datatypes_set_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb_run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifacts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0martifact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArtifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjob_builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msdk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0masyncio_compat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwb_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m from wandb.sdk.lib.import_hooks import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/wandb/sdk/internal/job_builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_artifact_name_safe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msettings_static\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSettingsStatic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0m_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}