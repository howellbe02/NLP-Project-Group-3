  & 'c:\Users\benja\.conda\envs\torchpip\python.exe' 'c:\Users\benja\.vscode\extensions\ms-python.debugpy-2025.6.0-win32-x64\bundled\libs\debugpy\launcher' '50814' '--' 'C:\Users\benja\OneDrive\Documents\GitHub\NLP-Project-Group-3\models\train_bigbird.py' 
Using device: cuda
Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Trainable parameters:
not  bert.embeddings.word_embeddings.weight
not  bert.embeddings.position_embeddings.weight
not  bert.embeddings.token_type_embeddings.weight
not  bert.embeddings.LayerNorm.weight
not  bert.embeddings.LayerNorm.bias
not  bert.encoder.layer.0.attention.self.query.weight
not  bert.encoder.layer.0.attention.self.query.bias
not  bert.encoder.layer.0.attention.self.key.weight
not  bert.encoder.layer.0.attention.self.key.bias
not  bert.encoder.layer.0.attention.self.value.weight
not  bert.encoder.layer.0.attention.self.value.bias
not  bert.encoder.layer.0.attention.output.dense.weight
not  bert.encoder.layer.0.attention.output.dense.bias
not  bert.encoder.layer.0.attention.output.LayerNorm.weight
not  bert.encoder.layer.0.attention.output.LayerNorm.bias
not  bert.encoder.layer.0.intermediate.dense.weight
not  bert.encoder.layer.0.intermediate.dense.bias
not  bert.encoder.layer.0.output.dense.weight
not  bert.encoder.layer.0.output.dense.bias
not  bert.encoder.layer.0.output.LayerNorm.weight
not  bert.encoder.layer.0.output.LayerNorm.bias
not  bert.encoder.layer.1.attention.self.query.weight
not  bert.encoder.layer.1.attention.self.query.bias
not  bert.encoder.layer.1.attention.self.key.weight
not  bert.encoder.layer.1.attention.self.key.bias
not  bert.encoder.layer.1.attention.self.value.weight
not  bert.encoder.layer.1.attention.self.value.bias
not  bert.encoder.layer.1.attention.output.dense.weight
not  bert.encoder.layer.1.attention.output.dense.bias
not  bert.encoder.layer.1.attention.output.LayerNorm.weight
not  bert.encoder.layer.1.attention.output.LayerNorm.bias
not  bert.encoder.layer.1.intermediate.dense.weight
not  bert.encoder.layer.1.intermediate.dense.bias
not  bert.encoder.layer.1.output.dense.weight
not  bert.encoder.layer.1.output.dense.bias
not  bert.encoder.layer.1.output.LayerNorm.weight
not  bert.encoder.layer.1.output.LayerNorm.bias
not  bert.encoder.layer.2.attention.self.query.weight
not  bert.encoder.layer.2.attention.self.query.bias
not  bert.encoder.layer.2.attention.self.key.weight
not  bert.encoder.layer.2.attention.self.key.bias
not  bert.encoder.layer.2.attention.self.value.weight
not  bert.encoder.layer.2.attention.self.value.bias
not  bert.encoder.layer.2.attention.output.dense.weight
not  bert.encoder.layer.2.attention.output.dense.bias
not  bert.encoder.layer.2.attention.output.LayerNorm.weight
not  bert.encoder.layer.2.attention.output.LayerNorm.bias
not  bert.encoder.layer.2.intermediate.dense.weight
not  bert.encoder.layer.2.intermediate.dense.bias
not  bert.encoder.layer.2.output.dense.weight
not  bert.encoder.layer.2.output.dense.bias
not  bert.encoder.layer.2.output.LayerNorm.weight
not  bert.encoder.layer.2.output.LayerNorm.bias
not  bert.encoder.layer.3.attention.self.query.weight
not  bert.encoder.layer.3.attention.self.query.bias
not  bert.encoder.layer.3.attention.self.key.weight
not  bert.encoder.layer.3.attention.self.key.bias
not  bert.encoder.layer.3.attention.self.value.weight
not  bert.encoder.layer.3.attention.self.value.bias
not  bert.encoder.layer.3.attention.output.dense.weight
not  bert.encoder.layer.3.attention.output.dense.bias
not  bert.encoder.layer.3.attention.output.LayerNorm.weight
not  bert.encoder.layer.3.attention.output.LayerNorm.bias
not  bert.encoder.layer.3.intermediate.dense.weight
not  bert.encoder.layer.3.intermediate.dense.bias
not  bert.encoder.layer.3.output.dense.weight
not  bert.encoder.layer.3.output.dense.bias
not  bert.encoder.layer.3.output.LayerNorm.weight
not  bert.encoder.layer.3.output.LayerNorm.bias
not  bert.encoder.layer.4.attention.self.query.weight
not  bert.encoder.layer.4.attention.self.query.bias
not  bert.encoder.layer.4.attention.self.key.weight
not  bert.encoder.layer.4.attention.self.key.bias
not  bert.encoder.layer.4.attention.self.value.weight
not  bert.encoder.layer.4.attention.self.value.bias
not  bert.encoder.layer.4.attention.output.dense.weight
not  bert.encoder.layer.4.attention.output.dense.bias
not  bert.encoder.layer.4.attention.output.LayerNorm.weight
not  bert.encoder.layer.4.attention.output.LayerNorm.bias
not  bert.encoder.layer.4.intermediate.dense.weight
not  bert.encoder.layer.4.intermediate.dense.bias
not  bert.encoder.layer.4.output.dense.weight
not  bert.encoder.layer.4.output.dense.bias
not  bert.encoder.layer.4.output.LayerNorm.weight
not  bert.encoder.layer.4.output.LayerNorm.bias
not  bert.encoder.layer.5.attention.self.query.weight
not  bert.encoder.layer.5.attention.self.query.bias
not  bert.encoder.layer.5.attention.self.key.weight
not  bert.encoder.layer.5.attention.self.key.bias
not  bert.encoder.layer.5.attention.self.value.weight
not  bert.encoder.layer.5.attention.self.value.bias
not  bert.encoder.layer.5.attention.output.dense.weight
not  bert.encoder.layer.5.attention.output.dense.bias
not  bert.encoder.layer.5.attention.output.LayerNorm.weight
not  bert.encoder.layer.5.attention.output.LayerNorm.bias
not  bert.encoder.layer.5.intermediate.dense.weight
not  bert.encoder.layer.5.intermediate.dense.bias
not  bert.encoder.layer.5.output.dense.weight
not  bert.encoder.layer.5.output.dense.bias
not  bert.encoder.layer.5.output.LayerNorm.weight
not  bert.encoder.layer.5.output.LayerNorm.bias
not  bert.encoder.layer.6.attention.self.query.weight
not  bert.encoder.layer.6.attention.self.query.bias
not  bert.encoder.layer.6.attention.self.key.weight
not  bert.encoder.layer.6.attention.self.key.bias
not  bert.encoder.layer.6.attention.self.value.weight
not  bert.encoder.layer.6.attention.self.value.bias
not  bert.encoder.layer.6.attention.output.dense.weight
not  bert.encoder.layer.6.attention.output.dense.bias
not  bert.encoder.layer.6.attention.output.LayerNorm.weight
not  bert.encoder.layer.6.attention.output.LayerNorm.bias
not  bert.encoder.layer.6.intermediate.dense.weight
not  bert.encoder.layer.6.intermediate.dense.bias
not  bert.encoder.layer.6.output.dense.weight
not  bert.encoder.layer.6.output.dense.bias
not  bert.encoder.layer.6.output.LayerNorm.weight
not  bert.encoder.layer.6.output.LayerNorm.bias
not  bert.encoder.layer.7.attention.self.query.weight
not  bert.encoder.layer.7.attention.self.query.bias
not  bert.encoder.layer.7.attention.self.key.weight
not  bert.encoder.layer.7.attention.self.key.bias
not  bert.encoder.layer.7.attention.self.value.weight
not  bert.encoder.layer.7.attention.self.value.bias
not  bert.encoder.layer.7.attention.output.dense.weight
not  bert.encoder.layer.7.attention.output.dense.bias
not  bert.encoder.layer.7.attention.output.LayerNorm.weight
not  bert.encoder.layer.7.attention.output.LayerNorm.bias
not  bert.encoder.layer.7.intermediate.dense.weight
not  bert.encoder.layer.7.intermediate.dense.bias
not  bert.encoder.layer.7.output.dense.weight
not  bert.encoder.layer.7.output.dense.bias
not  bert.encoder.layer.7.output.LayerNorm.weight
not  bert.encoder.layer.7.output.LayerNorm.bias
not  bert.encoder.layer.8.attention.self.query.weight
not  bert.encoder.layer.8.attention.self.query.bias
not  bert.encoder.layer.8.attention.self.key.weight
not  bert.encoder.layer.8.attention.self.key.bias
not  bert.encoder.layer.8.attention.self.value.weight
not  bert.encoder.layer.8.attention.self.value.bias
not  bert.encoder.layer.8.attention.output.dense.weight
not  bert.encoder.layer.8.attention.output.dense.bias
not  bert.encoder.layer.8.attention.output.LayerNorm.weight
not  bert.encoder.layer.8.attention.output.LayerNorm.bias
not  bert.encoder.layer.8.intermediate.dense.weight
not  bert.encoder.layer.8.intermediate.dense.bias
not  bert.encoder.layer.8.output.dense.weight
not  bert.encoder.layer.8.output.dense.bias
not  bert.encoder.layer.8.output.LayerNorm.weight
not  bert.encoder.layer.8.output.LayerNorm.bias
not  bert.encoder.layer.9.attention.self.query.weight
not  bert.encoder.layer.9.attention.self.query.bias
not  bert.encoder.layer.9.attention.self.key.weight
not  bert.encoder.layer.9.attention.self.key.bias
not  bert.encoder.layer.9.attention.self.value.weight
not  bert.encoder.layer.9.attention.self.value.bias
not  bert.encoder.layer.9.attention.output.dense.weight
not  bert.encoder.layer.9.attention.output.dense.bias
not  bert.encoder.layer.9.attention.output.LayerNorm.weight
not  bert.encoder.layer.9.attention.output.LayerNorm.bias
not  bert.encoder.layer.9.intermediate.dense.weight
not  bert.encoder.layer.9.intermediate.dense.bias
not  bert.encoder.layer.9.output.dense.weight
not  bert.encoder.layer.9.output.dense.bias
not  bert.encoder.layer.9.output.LayerNorm.weight
not  bert.encoder.layer.9.output.LayerNorm.bias
not  bert.encoder.layer.10.attention.self.query.weight
not  bert.encoder.layer.10.attention.self.query.bias
not  bert.encoder.layer.10.attention.self.key.weight
not  bert.encoder.layer.10.attention.self.key.bias
not  bert.encoder.layer.10.attention.self.value.weight
not  bert.encoder.layer.10.attention.self.value.bias
not  bert.encoder.layer.10.attention.output.dense.weight
not  bert.encoder.layer.10.attention.output.dense.bias
not  bert.encoder.layer.10.attention.output.LayerNorm.weight
not  bert.encoder.layer.10.attention.output.LayerNorm.bias
not  bert.encoder.layer.10.intermediate.dense.weight
not  bert.encoder.layer.10.intermediate.dense.bias
not  bert.encoder.layer.10.output.dense.weight
not  bert.encoder.layer.10.output.dense.bias
not  bert.encoder.layer.10.output.LayerNorm.weight
not  bert.encoder.layer.10.output.LayerNorm.bias
not  bert.encoder.layer.11.attention.self.query.weight
not  bert.encoder.layer.11.attention.self.query.bias
not  bert.encoder.layer.11.attention.self.key.weight
not  bert.encoder.layer.11.attention.self.key.bias
not  bert.encoder.layer.11.attention.self.value.weight
not  bert.encoder.layer.11.attention.self.value.bias
not  bert.encoder.layer.11.attention.output.dense.weight
not  bert.encoder.layer.11.attention.output.dense.bias
not  bert.encoder.layer.11.attention.output.LayerNorm.weight
not  bert.encoder.layer.11.attention.output.LayerNorm.bias
not  bert.encoder.layer.11.intermediate.dense.weight
not  bert.encoder.layer.11.intermediate.dense.bias
not  bert.encoder.layer.11.output.dense.weight
not  bert.encoder.layer.11.output.dense.bias
not  bert.encoder.layer.11.output.LayerNorm.weight
not  bert.encoder.layer.11.output.LayerNorm.bias
not  bert.pooler.weight
not  bert.pooler.bias
   classifier.dense.weight
   classifier.dense.bias
   classifier.out_proj.weight
   classifier.out_proj.bias
C:\Users\benja\OneDrive\Documents\GitHub\NLP-Project-Group-3\models\train_bigbird.py:172: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
{'loss': 1.2424, 'grad_norm': 3.406754970550537, 'learning_rate': 4.9040000000000005e-05, 'epoch': 0.2}
{'loss': 1.185, 'grad_norm': 3.044255018234253, 'learning_rate': 4.804e-05, 'epoch': 0.4}                                                                                           
{'loss': 1.1313, 'grad_norm': 8.471479415893555, 'learning_rate': 4.7040000000000004e-05, 'epoch': 0.6}                                                                             
{'loss': 1.2061, 'grad_norm': 4.344150066375732, 'learning_rate': 4.604e-05, 'epoch': 0.8}                                                                                          
{'loss': 1.1061, 'grad_norm': 8.102049827575684, 'learning_rate': 4.504e-05, 'epoch': 1.0}                                                                                          
{'eval_loss': 1.1146389245986938, 'eval_accuracy': 0.43, 'eval_f1': 0.15034965034965034, 'eval_recall': 0.25, 'eval_runtime': 28.421, 'eval_samples_per_second': 3.519, 'eval_steps_per_second': 0.88, 'epoch': 1.0}
{'loss': 1.143, 'grad_norm': 6.409655570983887, 'learning_rate': 4.4040000000000005e-05, 'epoch': 1.2}                                                                              
{'loss': 1.1477, 'grad_norm': 5.531246662139893, 'learning_rate': 4.304e-05, 'epoch': 1.4}
{'loss': 1.2426, 'grad_norm': 4.221912384033203, 'learning_rate': 4.2040000000000004e-05, 'epoch': 1.6}                                                                             
{'loss': 1.0094, 'grad_norm': 2.5747218132019043, 'learning_rate': 4.104e-05, 'epoch': 1.8}                                                                                         
{'loss': 1.1877, 'grad_norm': 3.9135491847991943, 'learning_rate': 4.004e-05, 'epoch': 2.0}                                                                                         
{'eval_loss': 1.1034823656082153, 'eval_accuracy': 0.52, 'eval_f1': 0.26945812807881775, 'eval_recall': 0.3023255813953488, 'eval_runtime': 27.3505, 'eval_samples_per_second': 3.656, 'eval_steps_per_second': 0.914, 'epoch': 2.0}
{'loss': 1.1235, 'grad_norm': 4.421112537384033, 'learning_rate': 3.9040000000000006e-05, 'epoch': 2.2}                                                                             
{'loss': 1.1545, 'grad_norm': 4.1994242668151855, 'learning_rate': 3.804e-05, 'epoch': 2.4}
{'loss': 1.0712, 'grad_norm': 2.166860342025757, 'learning_rate': 3.7040000000000005e-05, 'epoch': 2.6}                                                                             
{'loss': 1.215, 'grad_norm': 3.6876730918884277, 'learning_rate': 3.604e-05, 'epoch': 2.8}                                                                                          
{'loss': 1.1483, 'grad_norm': 2.7929883003234863, 'learning_rate': 3.504e-05, 'epoch': 3.0}                                                                                         
{'eval_loss': 1.1061984300613403, 'eval_accuracy': 0.44, 'eval_f1': 0.1627720870678617, 'eval_recall': 0.2558139534883721, 'eval_runtime': 27.5569, 'eval_samples_per_second': 3.629, 'eval_steps_per_second': 0.907, 'epoch': 3.0}
{'loss': 1.1137, 'grad_norm': 3.373959541320801, 'learning_rate': 3.404e-05, 'epoch': 3.2}                                                                                          
{'loss': 1.1567, 'grad_norm': 2.493617057800293, 'learning_rate': 3.304e-05, 'epoch': 3.4}
{'loss': 1.1397, 'grad_norm': 3.810987710952759, 'learning_rate': 3.2040000000000005e-05, 'epoch': 3.6}                                                                             
{'loss': 1.1187, 'grad_norm': 2.2804901599884033, 'learning_rate': 3.104e-05, 'epoch': 3.8}                                                                                         
{'loss': 1.1488, 'grad_norm': 6.612513065338135, 'learning_rate': 3.004e-05, 'epoch': 4.0}                                                                                          
{'eval_loss': 1.1081570386886597, 'eval_accuracy': 0.45, 'eval_f1': 0.1971677559912854, 'eval_recall': 0.2616279069767442, 'eval_runtime': 28.6407, 'eval_samples_per_second': 3.492, 'eval_steps_per_second': 0.873, 'epoch': 4.0}
{'loss': 1.0567, 'grad_norm': 3.117341995239258, 'learning_rate': 2.904e-05, 'epoch': 4.2}                                                                                          
{'loss': 1.1433, 'grad_norm': 4.126574993133545, 'learning_rate': 2.804e-05, 'epoch': 4.4}
{'loss': 1.2242, 'grad_norm': 4.176856517791748, 'learning_rate': 2.704e-05, 'epoch': 4.6}                                                                                          
{'loss': 1.127, 'grad_norm': 3.1771371364593506, 'learning_rate': 2.6040000000000005e-05, 'epoch': 4.8}                                                                             
{'loss': 1.1154, 'grad_norm': 4.534914970397949, 'learning_rate': 2.504e-05, 'epoch': 5.0}                                                                                          
{'eval_loss': 1.1118383407592773, 'eval_accuracy': 0.45, 'eval_f1': 0.1971677559912854, 'eval_recall': 0.2616279069767442, 'eval_runtime': 27.1142, 'eval_samples_per_second': 3.688, 'eval_steps_per_second': 0.922, 'epoch': 5.0}
{'loss': 1.0922, 'grad_norm': 4.327779769897461, 'learning_rate': 2.404e-05, 'epoch': 5.2}                                                                                          
{'loss': 1.097, 'grad_norm': 7.359999179840088, 'learning_rate': 2.304e-05, 'epoch': 5.4}
{'loss': 1.144, 'grad_norm': 7.045327186584473, 'learning_rate': 2.2040000000000002e-05, 'epoch': 5.6}                                                                              
{'loss': 1.1262, 'grad_norm': 3.996858835220337, 'learning_rate': 2.1040000000000002e-05, 'epoch': 5.8}                                                                             
{'loss': 1.172, 'grad_norm': 5.091214656829834, 'learning_rate': 2.004e-05, 'epoch': 6.0}                                                                                           
{'eval_loss': 1.1090304851531982, 'eval_accuracy': 0.45, 'eval_f1': 0.20322031493828913, 'eval_recall': 0.26162790697674415, 'eval_runtime': 27.129, 'eval_samples_per_second': 3.686, 'eval_steps_per_second': 0.922, 'epoch': 6.0}
{'loss': 1.1635, 'grad_norm': 3.896515130996704, 'learning_rate': 1.904e-05, 'epoch': 6.2}                                                                                          
{'loss': 1.1151, 'grad_norm': 3.425908088684082, 'learning_rate': 1.804e-05, 'epoch': 6.4}
{'loss': 1.1264, 'grad_norm': 7.527493953704834, 'learning_rate': 1.704e-05, 'epoch': 6.6}                                                                                          
{'loss': 1.1321, 'grad_norm': 7.490503311157227, 'learning_rate': 1.604e-05, 'epoch': 6.8}                                                                                          
{'loss': 1.0868, 'grad_norm': 2.126096487045288, 'learning_rate': 1.5040000000000002e-05, 'epoch': 7.0}                                                                             
{'eval_loss': 1.109959602355957, 'eval_accuracy': 0.53, 'eval_f1': 0.27486516994857646, 'eval_recall': 0.3081395348837209, 'eval_runtime': 27.3169, 'eval_samples_per_second': 3.661, 'eval_steps_per_second': 0.915, 'epoch': 7.0}
{'loss': 1.0612, 'grad_norm': 6.524023532867432, 'learning_rate': 1.4040000000000001e-05, 'epoch': 7.2}                                                                             
{'loss': 1.1438, 'grad_norm': 4.246248722076416, 'learning_rate': 1.3039999999999999e-05, 'epoch': 7.4}
{'loss': 1.207, 'grad_norm': 7.180049419403076, 'learning_rate': 1.204e-05, 'epoch': 7.6}                                                                                           
{'loss': 1.112, 'grad_norm': 3.3328866958618164, 'learning_rate': 1.1040000000000001e-05, 'epoch': 7.8}                                                                             
{'loss': 1.0614, 'grad_norm': 3.4124083518981934, 'learning_rate': 1.004e-05, 'epoch': 8.0}                                                                                         
{'eval_loss': 1.1112922430038452, 'eval_accuracy': 0.51, 'eval_f1': 0.2655146785581568, 'eval_recall': 0.2965116279069767, 'eval_runtime': 28.3686, 'eval_samples_per_second': 3.525, 'eval_steps_per_second': 0.881, 'epoch': 8.0}
{'loss': 1.0525, 'grad_norm': 3.2931418418884277, 'learning_rate': 9.04e-06, 'epoch': 8.2}                                                                                          
{'loss': 1.0921, 'grad_norm': 2.3192481994628906, 'learning_rate': 8.040000000000001e-06, 'epoch': 8.4}
{'loss': 1.1515, 'grad_norm': 3.569507598876953, 'learning_rate': 7.04e-06, 'epoch': 8.6}                                                                                           
{'loss': 1.2339, 'grad_norm': 3.146204710006714, 'learning_rate': 6.040000000000001e-06, 'epoch': 8.8}                                                                              
{'loss': 1.1027, 'grad_norm': 3.422001600265503, 'learning_rate': 5.04e-06, 'epoch': 9.0}                                                                                           
{'eval_loss': 1.112170934677124, 'eval_accuracy': 0.47, 'eval_f1': 0.2215822345593338, 'eval_recall': 0.27325581395348836, 'eval_runtime': 27.2903, 'eval_samples_per_second': 3.664, 'eval_steps_per_second': 0.916, 'epoch': 9.0}
{'loss': 1.1987, 'grad_norm': 4.283853530883789, 'learning_rate': 4.04e-06, 'epoch': 9.2}                                                                                           
{'loss': 1.0824, 'grad_norm': 3.649066209793091, 'learning_rate': 3.04e-06, 'epoch': 9.4}
{'loss': 1.0888, 'grad_norm': 2.8943684101104736, 'learning_rate': 2.0400000000000004e-06, 'epoch': 9.6}                                                                            
{'loss': 1.1419, 'grad_norm': 2.412968635559082, 'learning_rate': 1.04e-06, 'epoch': 9.8}                                                                                           
{'loss': 1.0784, 'grad_norm': 4.404349327087402, 'learning_rate': 4e-08, 'epoch': 10.0}                                                                                             
{'eval_loss': 1.1126254796981812, 'eval_accuracy': 0.46, 'eval_f1': 0.20694603903559128, 'eval_recall': 0.26744186046511625, 'eval_runtime': 27.319, 'eval_samples_per_second': 3.66, 'eval_steps_per_second': 0.915, 'epoch': 10.0}
{'train_runtime': 1687.8249, 'train_samples_per_second': 2.962, 'train_steps_per_second': 0.741, 'train_loss': 1.1344373413085937, 'epoch': 10.0}                                   
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1250/1250 [28:07<00:00,  1.35s/it]
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:25<00:00,  1.04s/it]
Evaluation results:
 {'eval_loss': 1.109959602355957, 'eval_accuracy': 0.53, 'eval_f1': 0.27486516994857646, 'eval_recall': 0.3081395348837209, 'eval_runtime': 27.1507, 'eval_samples_per_second': 3.683, 'eval_steps_per_second': 0.921, 'epoch': 10.0}